\documentclass[sigconf,review]{acmart}

% Remove ACM reference format
\setcitestyle{authoryear,round}

% Package for better figure handling
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tabularx}
\usepackage{booktabs}

% Metadata
\acmConference[Computer Graphics]{CSCE604029}{Semester Gasal 2025/2026}{Universitas Indonesia}
\acmYear{2025}
\acmDOI{10.1145/xxxxxx.xxxxxx}

% Copyright
\setcopyright{acmlicensed}
\copyrightyear{2025}

% Title and Authors
\title{Sketch-to-Material Studio: Integrating Classical Rendering with AI-Based Texture Generation}

\author{Carleano Ravelza Wongso}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{carleano.ravelza@ui.ac.id}
\orcid{0000-0000-0000-0000}

\author{Andrew Devito Aryo}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{andrew.devito@ui.ac.id}
\orcid{0000-0000-0000-0000}

\author{Arya Raditya Kusuma}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{arya.raditya@ui.ac.id}
\orcid{0000-0000-0000-0000}

\author{Tristan Agra Yudhistira}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{tristan.agra@ui.ac.id}
\orcid{0000-0000-0000-0000}

% Additional authors (if group has 4 members)
% \author{Author Four}
% \affiliation{%
%   \institution{Universitas Indonesia}
%   \city{Depok}
%   \country{Indonesia}
% }
% \email{author4@ui.ac.id}

% Abstract
\begin{abstract}
This paper presents Sketch-to-Material Studio, a computer graphics system that seamlessly integrates classical OpenGL rendering techniques with modern AI-based texture generation. Our system enables users to create detailed 3D materials from simple 2D sketches by leveraging Stable Diffusion ControlNet for sketch-conditioned texture synthesis and learned depth estimation for geometry extraction. The system automatically generates a complete PBR material set (albedo, depth, normal, and roughness maps) from user sketches, which are then rendered in real-time using a classical rasterization pipeline with Phong lighting, displacement mapping, and normal mapping. We demonstrate that the combination of ControlNet-based generative AI models and classical rendering techniques produces high-quality material visualization while maintaining interactive performance. Our approach addresses the challenge of rapid material prototyping in 3D graphics workflows, providing an intuitive interface that bridges the gap between artistic sketching and technical material creation. Experimental results show that our system can generate visually appealing materials with realistic lighting and surface details, achieving a balance between AI-driven content generation and classical rendering quality.
\end{abstract}

% Keywords
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010371.10010372</concept_id>
<concept_desc>Computing methodologies~Computer graphics</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010371.10010382</concept_id>
<concept_desc>Computing methodologies~Rendering</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010371.10010387</concept_id>
<concept_desc>Computing methodologies~Texture synthesis</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Computer graphics}
\ccsdesc[500]{Computing methodologies~Rendering}
\ccsdesc[300]{Computing methodologies~Texture synthesis}

\keywords{texture generation, stable diffusion, OpenGL rendering, depth estimation, material synthesis}

% Document
\begin{document}

\maketitle

% ==========================================
% 1. INTRODUCTION
% ==========================================
\section{Introduction}

The creation of realistic materials and textures is a fundamental challenge in computer graphics, requiring both artistic skill and technical expertise. Traditional workflows involve manual texture painting, photogrammetry, or procedural generation, each with significant time investments and technical barriers. Recent advances in generative AI, particularly diffusion models, have shown remarkable capability in synthesizing high-quality textures from text prompts or simple sketches. However, these AI-generated textures often lack the geometric context and lighting integration required for realistic 3D visualization.

This paper presents \textit{Sketch-to-Material Studio}, a system that bridges the gap between AI-driven texture generation and classical 3D rendering. Our approach combines Stable Diffusion ControlNet for sketch-conditioned texture synthesis with learned depth estimation to automatically generate complete PBR material sets (albedo, depth, normal, and roughness maps) from user sketches. The ControlNet architecture enables precise control over texture generation by conditioning on the input sketch structure, ensuring that generated textures preserve the user's artistic intent. These materials are then rendered using classical OpenGL techniques, including Phong lighting, displacement mapping, and normal mapping, to produce interactive 3D visualizations.

Our main contributions include an integrated pipeline that combines ControlNet-based diffusion texture generation with classical rendering, automatic PBR material generation (albedo, depth, normal, and roughness maps) from sketches, automatic geometry extraction from generated textures using learned depth estimation, a real-time interactive system for material visualization with threaded AI loading, and a demonstration of effective integration between modern AI techniques and classical graphics algorithms.

% ==========================================
% 2. LITERATURE REVIEW
% ==========================================
\section{Literature Review}

\textbf{Classical Rendering Techniques.} Classical computer graphics has established robust methods for material representation and rendering. The Phong lighting model \cite{phong1975illumination} provides a computationally efficient approximation of surface illumination through ambient, diffuse, and specular components. Texture mapping, introduced by Catmull \cite{catmull1974subdivision}, enables detailed surface appearance without geometric complexity. Displacement mapping \cite{cook1984shade} extends this concept by modifying surface geometry based on texture values, creating more realistic surface details.

Modern GPU-based rendering pipelines leverage these classical techniques through programmable shaders, enabling real-time rendering of complex materials. Normal mapping \cite{blinn1978simulation} further enhances surface detail by perturbing surface normals without geometric modification, providing an efficient method for fine-scale surface variation.

\textbf{AI-Based Graphics Generation.} Recent advances in deep learning have revolutionized texture and material generation. Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} enabled high-quality image synthesis, while Variational Autoencoders (VAEs) \cite{kingma2013auto} provided structured latent representations. However, diffusion models \cite{ho2020denoising, rombach2022high} have emerged as the state-of-the-art for image generation, offering superior quality and controllability.

Stable Diffusion \cite{rombach2022high} specifically enables text-to-image and image-to-image generation through a latent diffusion process, making it accessible for texture synthesis applications. ControlNet \cite{zhang2023adding} extends Stable Diffusion by adding conditional control mechanisms, enabling precise control over generation through various conditioning inputs such as edge maps, depth maps, or scribbles. The scribble ControlNet variant is particularly suitable for sketch-based texture generation, as it preserves the structural information from user sketches while allowing the model to add realistic texture details.

\textbf{Depth Estimation and Geometry Extraction.} Monocular depth estimation has been extensively studied in computer vision. Early methods relied on hand-crafted features and geometric constraints \cite{saxena2009make3d}. Deep learning approaches, such as those by Eigen et al. \cite{eigen2015predicting} and Godard et al. \cite{godard2017unsupervised}, have significantly improved accuracy. Recent models like Depth-Anything \cite{yang2024depth} provide robust depth estimation suitable for geometry extraction in graphics applications.

Normal maps can be derived from depth maps through gradient computation, as demonstrated in classical computer vision \cite{horn1981determining}. This approach enables automatic normal map generation from estimated depth, completing the material set required for realistic rendering.

\textbf{Integration of AI and Classical Graphics.} Several works have explored combining AI generation with classical rendering. Neural rendering approaches like NeRF \cite{mildenhall2020nerf} use learned representations but require extensive training. Our approach differs by using pre-trained generative models and integrating their output with classical rendering pipelines, providing flexibility and real-time performance.

% ==========================================
% 3. SYSTEM DESIGN
% ==========================================
\section{System Design}

\textbf{Architecture Overview.} Our system follows a three-stage pipeline: (1) \textit{Sketch Input}, where users create or upload 2D sketches, (2) \textit{AI Generation}, which produces material maps from sketches, and (3) \textit{Classical Rendering}, which visualizes the materials in 3D. The complete system pipeline is illustrated in Figure~\ref{fig:pipeline}.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/pipeline.png}
\caption{System pipeline from sketch to 3D rendering.}
\label{fig:pipeline}
\end{figure*}

\subsection{Sketch Input}
The sketch input module provides an interactive 2D canvas implemented using PyGame. Users can draw directly with mouse input, select colors from a palette, or drag-and-drop existing images. The canvas supports real-time drawing with configurable brush sizes and color selection. Sketches are saved as 512x512 pixel images for processing by the AI pipeline.

\subsection{AI Generation Pipeline}
The AI generation stage consists of three primary components that transform the 2D sketch into a comprehensive set of material maps:

\textbf{Texture Synthesis.}
We employ Stable Diffusion v1.5 with ControlNet (scribble variant) to generate detailed textures from user sketches. The ControlNet architecture provides precise structural control by conditioning the diffusion process on the input sketch. Sketches are preprocessed through binary thresholding and inversion to create clean edge maps that serve as ControlNet conditioning. The model uses UniPCMultistepScheduler for efficient inference with 30 steps. ControlNet conditioning scale is set to 0.7 to balance between preserving sketch structure and allowing creative freedom. Positive prompts emphasize "seamless texture, top down view, flat lighting, 8k, highly detailed, photorealistic material" while negative prompts exclude "perspective, 3d, shadows, people, objects, text, watermark, blurry, low quality, distortion". Guidance scale is set to 8.0 for strong prompt adherence.

\textbf{Depth Estimation.}
We use the Depth-Anything-Small model \cite{yang2024depth} to estimate depth from the generated texture. This provides a depth map that encodes geometric information for displacement mapping.

\textbf{Normal Map Generation.}
Normal maps are computed from depth maps using Sobel operators (kernel size 5) to compute gradients. The gradients are normalized and encoded in RGB format, providing surface normal perturbations for enhanced lighting detail.

\textbf{Roughness Map Generation.}
Roughness maps are automatically derived from the generated albedo texture by converting to grayscale, normalizing, and inverting the values. This approach assumes that darker regions in the albedo correspond to rougher surfaces, providing a physically plausible roughness distribution without requiring additional AI processing.

\subsection{Classical Rendering}
The rendering module implements a classical OpenGL 3.3 rasterization pipeline to visualize the generated maps in real-time:

\textbf{Geometry.}
A 200x200 grid mesh serves as the base geometry, providing high-resolution surface detail for displacement mapping. Vertices include position, texture coordinates, and normals, stored in a Vertex Array Object (VAO) for efficient GPU access.

\textbf{Vertex Shader.}
Performs displacement mapping by sampling the depth texture and displacing vertices along their normals. The displacement strength is configurable (default 0.3) to control surface relief.

\textbf{Fragment Shader.}
Implements an enhanced Phong lighting model with four key components:
\begin{itemize}
    \item \textit{Ambient}: Constant 0.05 contribution to prevent total darkness in shadowed areas.
    \item \textit{Diffuse}: Lambertian reflection based on surface normal and light direction.
    \item \textit{Specular}: Blinn-Phong highlights with dynamic shininess controlled by the roughness map.
    \item \textit{Rim Light}: Fresnel-like effect to accentuate object silhouettes and enhance 3D readability.
\end{itemize}

The shader samples four textures: albedo for base color, depth for displacement, normal for surface detail, and roughness for specular control. Normal maps are blended with geometric normals to enhance surface detail. Roughness values modulate the specular highlight intensity and sharpness, creating more realistic material appearance.

\textbf{Camera System.}
An orbital camera allows interactive viewing with mouse drag controls. The camera uses spherical coordinates converted to Cartesian for LookAt matrix computation.

% ==========================================
% 4. IMPLEMENTATION
% ==========================================
% ... (Bagian Rasterization Pipeline tetap sama) ...

\textbf{Lighting Model.}
The lighting calculation is implemented in the fragment shader using an extended Blinn-Phong model that includes a Fresnel-based rim term. The final pixel intensity $I$ is calculated as:

\begin{equation}
I = I_a + I_d + I_s + I_r
\end{equation}

Where the components are defined as:
\begin{align*}
I_a &= 0.05 \cdot \mathbf{A} \\
I_d &= \max(\mathbf{N} \cdot \mathbf{L}, 0) \cdot \mathbf{A} \\
I_s &= 0.3 \cdot (1 - \text{rough}) \cdot (\mathbf{N} \cdot \mathbf{H})^{\alpha} \\
I_r &= 0.3 \cdot (1 - \max(\mathbf{V} \cdot \mathbf{N}, 0))^3
\end{align*}

Here, $\mathbf{A}$ is the albedo color, $\mathbf{N}$ is the perturbed surface normal, $\mathbf{L}$ is the light direction, $\mathbf{H}$ is the half-vector, and $\mathbf{V}$ is the view direction. 

Unlike standard Phong shading with a fixed exponent, our implementation uses a **dynamic shininess exponent** $\alpha$ derived from the roughness map:
\begin{equation}
\alpha = \text{mix}(2.0, 64.0, 1.0 - \text{rough})
\end{equation}
This ensures that smoother surfaces (low roughness) exhibit sharper highlights (higher $\alpha$), while rougher surfaces produce broader, dimmer highlights, accurately simulating physical material properties.

\subsection{AI-Based Components}

\textbf{Stable Diffusion ControlNet Integration.}
We use the Hugging Face Diffusers library to load and run Stable Diffusion v1.5 with ControlNet (scribble variant). The model supports both CUDA (NVIDIA GPUs), MPS (Apple Silicon), and CPU execution, with automatic device selection. The ControlNet pipeline accepts:
\begin{itemize}
    \item Input sketch preprocessed through binary thresholding and inversion.
    \item Text prompt for generation guidance.
    \item ControlNet conditioning scale (0.7) controlling sketch structure preservation.
    \item Guidance scale (8.0) for prompt adherence.
    \item UniPCMultistepScheduler for efficient 30-step inference.
\end{itemize}
The AI pipeline is loaded in a separate thread to prevent blocking the user interface during initialization, which can take several minutes on first run.

\textbf{Depth Estimation.}
The Depth-Anything-Small model is loaded via Hugging Face Transformers pipeline API. The model processes the generated texture and outputs a depth map, which is resized to match the texture resolution (512x512).

\textbf{Normal Map Computation.}
Normal maps are computed using OpenCV's Sobel operators with kernel size 5:
\begin{equation}
\mathbf{N} = \text{normalize}([-S_x, -S_y, k])
\end{equation}
where $S_x$ and $S_y$ are Sobel gradients, and $k$ is a constant (500.0) representing the base normal component. The result is normalized and encoded to [0, 255] range for texture storage.

\textbf{Roughness Map Computation.}
Roughness maps are derived from albedo textures through grayscale conversion and inversion:
\begin{equation}
R = 255 - \text{normalize}(\text{grayscale}(A))
\end{equation}
where $A$ is the albedo texture. This assumes darker albedo regions correspond to rougher surfaces, providing a physically plausible approximation.

\subsection{System Integration}
The system integrates AI and classical components through a two-mode interface:
\begin{itemize}
    \item \textit{Paint Mode}: 2D sketch creation using PyGame.
    \item \textit{View Mode}: 3D visualization using OpenGL.
\end{itemize}

Mode switching occurs automatically after AI generation completes. The system handles blocking AI operations gracefully through threaded loading and progress callbacks. A project management system saves all generated materials (albedo, depth, normal, roughness) in organized project folders, enabling users to reload previous work. The interface includes template loading, project browsing, and real-time progress feedback during generation.

% ==========================================
% 5. EVALUATION
% ==========================================
\section{Evaluation}

\subsection{Visual Quality Assessment}
We evaluate the visual quality of generated materials through qualitative analysis. Figure~\ref{fig:results} illustrates the generation pipeline results. The system successfully translates a binary sketch into a rich albedo texture using Stable Diffusion, which is then processed into PBR maps to produce a realistic 3D render with depth displacement and lighting.

\begin{figure}[h]
\centering
% 1. Input Sketch
\begin{subfigure}{0.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{figures/sketch_input.png} % Pastikan nama file sesuai
\caption{Input Sketch}
\end{subfigure}
\hfill
% 2. Albedo Map (Hasil Diffusion)
\begin{subfigure}{0.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{figures/albedo.png} % Pastikan nama file sesuai
\caption{Generated Albedo}
\end{subfigure}
\hfill
% 3. Final 3D Render
\begin{subfigure}{0.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{figures/final_render.png} % Ganti dengan nama file screenshot 3D kamu
\caption{Final 3D Render}
\end{subfigure}

\caption{Visual generation process: (a) User input sketch, (b) Albedo texture generated by Stable Diffusion ControlNet, and (c) Final interactive 3D rendering demonstrating displacement and Phong lighting.}
\label{fig:results}
\end{figure}

The Phong lighting model provides realistic illumination, with specular highlights enhanced by roughness map modulation. Displacement mapping creates visible geometric relief based on the depth extracted from the albedo, while normal mapping adds fine-scale surface variation. The combination produces materials that appear three-dimensional and realistic based on the initial 2D input.

\subsection{Performance Metrics}
Performance measurements were conducted on multiple hardware configurations:

\begin{table}[h]
\centering
\caption{Generation time for different hardware configurations}
\label{tab:performance}
\footnotesize
\begin{tabularx}{\columnwidth}{lXXXX}
\toprule
\textbf{Hardware} & \textbf{Albedo} & \textbf{Roughness} & \textbf{Depth} & \textbf{Normal} \\
\midrule
NVIDIA RTX 3080 (CUDA) & 35s & 8s & 11s & 15s \\
Apple M1 Pro (MPS) & 65s & 15s & 20s & 25s \\
Intel i5-11300H (CPU) & 264s & 0.03s & 0.08s & 0.15s \\
Intel i7-10700K (CPU) & 420s & 45s & 60s & 75s \\
\bottomrule
\end{tabularx}
\end{table}

Rendering performance achieves 60 FPS on all tested configurations, as the classical rendering pipeline is GPU-accelerated and computationally efficient. The AI generation stage is the primary bottleneck, but remains acceptable for interactive use on GPU-enabled systems.

% ==========================================
% 6. DISCUSSION
% ==========================================
\section{Discussion}

\subsection{Findings}
Our implementation demonstrates that modern AI models can be effectively integrated with classical rendering pipelines. The combination of ControlNet-based Stable Diffusion for texture generation and learned depth estimation provides a complete PBR material creation workflow. The classical rendering pipeline ensures real-time performance and realistic lighting, while AI components handle the complex task of content generation.

Key observations include:
\begin{itemize}
    \item ControlNet effectively preserves sketch structure while adding realistic detail, providing better structural control than standard Img2Img approaches.
    \item Depth estimation provides sufficient geometric information for convincing displacement mapping.
    \item Normal maps derived from depth enhance surface detail without additional computation.
    \item Roughness maps derived from albedo provide physically plausible material properties without requiring additional AI processing.
    \item The Phong lighting model, enhanced with roughness modulation, produces visually appealing results when combined with high-quality textures.
    \item Threaded AI loading prevents UI blocking during model initialization, improving user experience.
\end{itemize}

\subsection{Limitations}
Several limitations are present in the current implementation:

\textbf{Generation Time.}
AI generation requires 30-60 seconds on GPU hardware, which may interrupt workflow for rapid iteration. CPU execution is significantly slower (5-10 minutes), limiting accessibility.

\textbf{Resolution Constraints.}
Textures are generated at 512x512 resolution due to model constraints. Higher resolutions would improve quality but require more computational resources.

\textbf{Blocking Operations.}
While initialization is threaded, the current implementation blocks the UI during the specific generation phase. Asynchronous processing would improve user experience.

\textbf{Limited Control.}
Users have limited control over generation parameters. More granular control over texture style, detail level, and geometric properties would enhance usability.

\textbf{Lighting Constraints.}
The system uses a fixed lighting setup. Dynamic lighting or multiple light sources would provide more realistic visualization.

% ==========================================
% 7. FUTURE WORK
% ==========================================
\section{Future Work}

Several directions can guide future improvements to the system.

\textbf{Performance Optimization}
Performance can be enhanced by implementing asynchronous AI generation using threading, introducing texture caching to avoid unnecessary regeneration, and optimizing both model loading and inference times.

\textbf{Enhanced Features}
Enhancements could include support for higher resolution texture generation, the addition of multiple lighting setups and dynamic lighting, and enabling users to edit material parameters such as roughness and metallic properties. Providing export functionality for integration with game engines like Unity or Unreal, along with animation support for material sequences, would further improve usability.

\textbf{Advanced AI Integration}
Advances in AI integration may involve fine-tuning models for specific material types, adding style transfer capabilities, and ensuring multi-view consistency in 3D texture generation. Incorporating methods such as NeRF or Gaussian Splatting would enable novel view synthesis directly from generated materials.

\textbf{User Experience}
User experience can be improved by developing a real-time preview of material generation, allowing brush-based editing of generated textures, and providing a material library with presets. Additionally, incorporating collaborative features would facilitate workflows for teams.


% ==========================================
% 8. CONCLUSION
% ==========================================
\section{Conclusion}

We have presented Sketch-to-Material Studio, a system that successfully integrates classical OpenGL rendering with modern AI-based texture generation. Our approach demonstrates that ControlNet-based diffusion models and learned depth estimation can be effectively combined with classical graphics techniques to create a complete PBR material creation workflow. The system enables users to generate high-quality 3D materials (albedo, depth, normal, and roughness maps) from simple sketches, with real-time visualization through classical rendering pipelines. The use of ControlNet ensures that generated textures preserve the structural intent of user sketches while adding realistic detail.

The integration of AI and classical components provides complementary strengths: AI handles complex content generation while classical techniques ensure real-time performance and realistic lighting. This combination addresses the challenge of rapid material prototyping in 3D graphics workflows, making material creation more accessible to artists and developers.

Future work will focus on performance optimization, enhanced user controls, and advanced AI integration to further improve the system's capabilities and usability.

% ==========================================
% 9. REFERENCES
% ==========================================
\begin{thebibliography}{99}

\bibitem{phong1975illumination}
Bui Tuong Phong.
\newblock Illumination for computer generated pictures.
\newblock \textit{Communications of the ACM}, 18(6):311--317, 1975.

\bibitem{catmull1974subdivision}
Edwin Catmull.
\newblock A subdivision algorithm for computer display of curved surfaces.
\newblock Technical report, University of Utah, 1974.

\bibitem{cook1984shade}
Robert L. Cook.
\newblock Shade trees.
\newblock In \textit{Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques}, SIGGRAPH '84, pages 223--231, 1984.

\bibitem{blinn1978simulation}
James F. Blinn.
\newblock Simulation of wrinkled surfaces.
\newblock In \textit{Proceedings of the 5th Annual Conference on Computer Graphics and Interactive Techniques}, SIGGRAPH '78, pages 286--292, 1978.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \textit{Advances in Neural Information Processing Systems}, volume 27, 2014.

\bibitem{kingma2013auto}
Diederik P. Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \textit{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \textit{Advances in Neural Information Processing Systems}, volume 33, pages 6840--6851, 2020.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{zhang2023adding}
Lvmin Zhang and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.
\newblock \textit{arXiv preprint arXiv:2302.05543}, 2023.

\bibitem{saxena2009make3d}
Ashutosh Saxena, Min Sun, and Andrew Y. Ng.
\newblock Make3d: Learning 3d scene structure from a single still image.
\newblock \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 31(5):824--840, 2009.

\bibitem{eigen2015predicting}
David Eigen, Christian Puhrsch, and Rob Fergus.
\newblock Depth map prediction from a single image using a multi-scale deep network.
\newblock In \textit{Advances in Neural Information Processing Systems}, volume 27, 2015.

\bibitem{godard2017unsupervised}
Clément Godard, Oisin Mac Aodha, and Gabriel J. Brostow.
\newblock Unsupervised monocular depth estimation with left-right consistency.
\newblock In \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 270--279, 2017.

\bibitem{yang2024depth}
Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao.
\newblock Depth anything: Unleashing the power of large-scale unlabeled data.
\newblock \textit{arXiv preprint arXiv:2401.10891}, 2024.

\bibitem{horn1981determining}
Berthold K. P. Horn.
\newblock Determining lightness from an image.
\newblock \textit{Computer Graphics and Image Processing}, 3(4):277--299, 1981.

\bibitem{mildenhall2020nerf}
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.
\newblock NeRF: Representing scenes as neural radiance fields for view synthesis.
\newblock In \textit{European Conference on Computer Vision}, pages 405--421, 2020.

\end{thebibliography}

\end{document}

