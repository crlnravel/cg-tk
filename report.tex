\documentclass[sigconf,review]{acmart}

% Remove ACM reference format
\setcitestyle{authoryear,round}

% Disable ACM copyright and reference block
\settopmatter{printacmref=false}
\settopmatter{printccs=false}
\settopmatter{printfolios=true}

\setcopyright{none}

% Package for better figure handling
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tabularx}
\usepackage{booktabs}

% Metadata
\acmConference[Computer Graphics]{CSCE604029}{Semester Gasal 2025/2026}{Universitas Indonesia}
\acmYear{2025}

% Title and Authors
\title{Sketch-to-Material Studio: Integrating Classical Rendering with AI-Based Texture Generation}

\author{Carleano Ravelza Wongso}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{carleano.ravelza@ui.ac.id}
\orcid{0000-0000-0000-0000}

\author{Andrew Devito Aryo}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{andrew.devito@ui.ac.id}
\orcid{0000-0000-0000-0000}

\author{Arya Raditya Kusuma}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{arya.raditya@ui.ac.id}
\orcid{0000-0000-0000-0000}

\author{Tristan Agra Yudhistira}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{tristan.agra@ui.ac.id}
\orcid{0000-0000-0000-0000}

% Additional authors (if group has 4 members)
% \author{Author Four}
% \affiliation{%
%   \institution{Universitas Indonesia}
%   \city{Depok}
%   \country{Indonesia}
% }
% \email{author4@ui.ac.id}

% Abstract
\begin{abstract}
This paper presents Sketch-to-Material Studio, a computer graphics system that seamlessly integrates classical OpenGL rendering techniques with modern AI-based texture generation. Our system enables users to create detailed 3D materials from simple 2D sketches by leveraging Stable Diffusion ControlNet for sketch-conditioned texture synthesis and learned depth estimation for geometry extraction. The system automatically generates a complete PBR material set (albedo, depth, normal, and roughness maps) from user sketches, which are then rendered in real-time using a classical rasterization pipeline with Phong lighting, displacement mapping, and normal mapping. We demonstrate that the combination of ControlNet-based generative AI models and classical rendering techniques produces high-quality material visualization while maintaining interactive performance. Our approach addresses the challenge of rapid material prototyping in 3D graphics workflows, providing an intuitive interface that bridges the gap between artistic sketching and technical material creation. Experimental results show that our system can generate visually appealing materials with realistic lighting and surface details, achieving a balance between AI-driven content generation and classical rendering quality.
\end{abstract}

% Keywords
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010371.10010372</concept_id>
<concept_desc>Computing methodologies~Computer graphics</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010371.10010382</concept_id>
<concept_desc>Computing methodologies~Rendering</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010371.10010387</concept_id>
<concept_desc>Computing methodologies~Texture synthesis</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Computer graphics}
\ccsdesc[500]{Computing methodologies~Rendering}
\ccsdesc[300]{Computing methodologies~Texture synthesis}

\keywords{texture generation, stable diffusion, OpenGL rendering, depth estimation, material synthesis}

% Document
\begin{document}

\maketitle

% ==========================================
% 1. INTRODUCTION
% ==========================================
\section{Introduction}

The creation of realistic materials and textures is a fundamental challenge in computer graphics, requiring both artistic skill and technical expertise. Traditional workflows involve manual texture painting, photogrammetry, or procedural generation, each with significant time investments and technical barriers. Recent advances in generative AI, particularly diffusion models, have shown remarkable capability in synthesizing high-quality textures from text prompts or simple sketches. However, these AI-generated textures often lack the geometric context and lighting integration required for realistic 3D visualization.

This paper presents \textit{Sketch-to-Material Studio}, a system that bridges the gap between AI-driven texture generation and classical 3D rendering. Our approach combines Stable Diffusion ControlNet for sketch-conditioned texture synthesis with learned depth estimation to automatically generate complete PBR material sets (albedo, depth, normal, and roughness maps) from user sketches. The ControlNet architecture enables precise control over texture generation by conditioning on the input sketch structure, ensuring that generated textures preserve the user's artistic intent. These materials are then rendered using classical OpenGL techniques, including Phong lighting, displacement mapping, and normal mapping, to produce interactive 3D visualizations.

Our main contributions include an integrated pipeline that combines ControlNet-based diffusion texture generation with classical rendering, automatic PBR material generation (albedo, depth, normal, and roughness maps) from sketches, automatic geometry extraction from generated textures using learned depth estimation, a real-time interactive system for material visualization with threaded AI loading, and a demonstration of effective integration between modern AI techniques and classical graphics algorithms.

% ==========================================
% 2. LITERATURE REVIEW
% ==========================================
\section{Literature Review}

\textbf{Classical Rendering Techniques.} Classical computer graphics has established robust methods for material representation and rendering. The Phong lighting model \cite{phong1975illumination} provides a computationally efficient approximation of surface illumination through ambient, diffuse, and specular components. Texture mapping, introduced by Catmull \cite{catmull1974subdivision}, enables detailed surface appearance without geometric complexity. Displacement mapping \cite{cook1984shade} extends this concept by modifying surface geometry based on texture values, creating more realistic surface details.

Modern GPU-based rendering pipelines leverage these classical techniques through programmable shaders, enabling real-time rendering of complex materials. Normal mapping \cite{blinn1978simulation} further enhances surface detail by perturbing surface normals without geometric modification, providing an efficient method for fine-scale surface variation.

\textbf{AI-Based Graphics Generation.} Recent advances in deep learning have revolutionized texture and material generation. Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} enabled high-quality image synthesis, while Variational Autoencoders (VAEs) \cite{kingma2013auto} provided structured latent representations. However, diffusion models \cite{ho2020denoising, rombach2022high} have emerged as the state-of-the-art for image generation, offering superior quality and controllability.

Stable Diffusion \cite{rombach2022high} specifically enables text-to-image and image-to-image generation through a latent diffusion process, making it accessible for texture synthesis applications. ControlNet \cite{zhang2023adding} extends Stable Diffusion by adding conditional control mechanisms, enabling precise control over generation through various conditioning inputs such as edge maps, depth maps, or scribbles. The scribble ControlNet variant is particularly suitable for sketch-based texture generation, as it preserves the structural information from user sketches while allowing the model to add realistic texture details.

\textbf{Depth Estimation and Geometry Extraction.} Monocular depth estimation has been extensively studied in computer vision. Early methods relied on hand-crafted features and geometric constraints \cite{saxena2009make3d}. Deep learning approaches, such as those by Eigen et al. \cite{eigen2015predicting} and Godard et al. \cite{godard2017unsupervised}, have significantly improved accuracy. Recent models like Depth-Anything \cite{yang2024depth} provide robust depth estimation suitable for geometry extraction in graphics applications.

Normal maps can be derived from depth maps through gradient computation, as demonstrated in classical computer vision \cite{horn1981determining}. This approach enables automatic normal map generation from estimated depth, completing the material set required for realistic rendering.

\textbf{Integration of AI and Classical Graphics.} Several works have explored combining AI generation with classical rendering. Neural rendering approaches like NeRF \cite{mildenhall2020nerf} use learned representations but require extensive training. Our approach differs by using pre-trained generative models and integrating their output with classical rendering pipelines, providing flexibility and real-time performance.

% ==========================================
% 3. SYSTEM DESIGN
% ==========================================
\section{System Design}

\textbf{Architecture Overview.} Our system follows a three-stage pipeline: (1) \textit{Sketch Input}, where users create or upload 2D sketches, (2) \textit{AI Generation}, which produces material maps from sketches, and (3) \textit{Classical Rendering}, which visualizes the materials in 3D. The complete system pipeline is illustrated in Figure~\ref{fig:pipeline}.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/pipeline.png}
\caption{System pipeline from sketch to 3D rendering.}
\label{fig:pipeline}
\end{figure*}

\subsection{Sketch Input}
The sketch input module provides an interactive 2D canvas implemented using PyGame. Users can draw directly with mouse input, select colors from a palette, or drag-and-drop existing images. The canvas supports real-time drawing with configurable brush sizes and color selection. Sketches are saved as 512x512 pixel images for processing by the AI pipeline.

\subsection{AI Generation Pipeline}
The AI generation stage consists of three primary components that transform the 2D sketch into a comprehensive set of material maps:

\textbf{Texture Synthesis.}
We employ Stable Diffusion v1.5 with ControlNet (scribble variant) to generate detailed textures from user sketches. The ControlNet architecture provides precise structural control by conditioning the diffusion process on the input sketch. Sketches are preprocessed through binary thresholding and inversion to create clean edge maps that serve as ControlNet conditioning. The model uses UniPCMultistepScheduler for efficient inference with 30 steps. ControlNet conditioning scale is set to 0.7 to balance between preserving sketch structure and allowing creative freedom. Positive prompts emphasize "seamless texture, top down view, flat lighting, 8k, highly detailed, photorealistic material" while negative prompts exclude "perspective, 3d, shadows, people, objects, text, watermark, blurry, low quality, distortion". Guidance scale is set to 8.0 for strong prompt adherence.

\textbf{Depth Estimation.}
We use the Depth-Anything-Small model \cite{yang2024depth} to estimate depth from the generated texture. This provides a depth map that encodes geometric information for displacement mapping.

\textbf{Normal Map Generation.}
Normal maps are computed from depth maps using Sobel operators (kernel size 5) to compute gradients. The gradients are normalized and encoded in RGB format, providing surface normal perturbations for enhanced lighting detail.

\textbf{Roughness Map Generation.}
Roughness maps are automatically derived from the generated albedo texture by converting to grayscale, normalizing, and inverting the values. This approach assumes that darker regions in the albedo correspond to rougher surfaces, providing a physically plausible roughness distribution without requiring additional AI processing.

\subsection{Classical Rendering}
The rendering module implements a classical OpenGL 3.3 rasterization pipeline to visualize the generated maps in real-time:

\textbf{Geometry.}
A 200x200 grid mesh serves as the base geometry, providing high-resolution surface detail for displacement mapping. Vertices include position, texture coordinates, and normals, stored in a Vertex Array Object (VAO) for efficient GPU access.

\textbf{Vertex Shader.}
Performs displacement mapping by sampling the depth texture and displacing vertices along their normals. The displacement strength is configurable (default 0.3) to control surface relief.

\textbf{Fragment Shader.}
Implements Phong lighting with three components:
\begin{itemize}
    \item \textit{Ambient}: Constant 0.2 contribution for base illumination
    \item \textit{Diffuse}: Lambertian reflection based on surface normal and light direction
    \item \textit{Specular}: Blinn-Phong specular highlights with shininess exponent 32
\end{itemize}

The shader samples four textures: albedo for base color, depth for displacement, normal for surface detail, and roughness for specular control. Normal maps are blended with geometric normals to enhance surface detail. Roughness values modulate the specular highlight intensity, creating more realistic material appearance.

\textbf{Camera System.}
An orbital camera allows interactive viewing with mouse drag controls. The camera uses spherical coordinates converted to Cartesian for LookAt matrix computation.

% ==========================================
% 4. IMPLEMENTATION
% ==========================================
\section{Implementation}

\subsection{Classical Graphics Components}

\textbf{Rasterization Pipeline.}
We implement a complete OpenGL rasterization pipeline using PyOpenGL. The pipeline includes:
\begin{itemize}
    \item Vertex Buffer Objects (VBO) for vertex data storage.
    \item Element Buffer Objects (EBO) for indexed rendering.
    \item Vertex Array Objects (VAO) for state management.
    \item Custom GLSL shaders for vertex and fragment processing.
\end{itemize}

\textbf{Lighting Model.}
The Phong lighting model is implemented in the fragment shader with the following equation:
\begin{equation}
I = I_a + I_d \cdot \max(\mathbf{N} \cdot \mathbf{L}, 0) + I_s \cdot (\mathbf{N} \cdot \mathbf{H})^n
\end{equation}
where $I_a$, $I_d$, and $I_s$ are ambient, diffuse, and specular intensities, $\mathbf{N}$ is the surface normal, $\mathbf{L}$ is the light direction, $\mathbf{H}$ is the half-vector, and $n$ is the shininess exponent.

\textbf{Texture Mapping.}
Multi-texture binding enables simultaneous use of albedo, depth, normal, and roughness maps. Textures are loaded using PIL/Pillow and uploaded to GPU memory with mipmap generation for quality filtering. The system organizes generated materials into project folders for easy management and reuse.

\textbf{Displacement Mapping.}
Vertex displacement is performed in the vertex shader by sampling the depth texture and scaling the displacement by a configurable strength parameter. This creates geometric detail without modifying the base mesh topology.

\subsection{AI-Based Components}

\textbf{Stable Diffusion ControlNet Integration.}
We use the Hugging Face Diffusers library to load and run Stable Diffusion v1.5 with ControlNet (scribble variant). The model supports both CUDA (NVIDIA GPUs), MPS (Apple Silicon), and CPU execution, with automatic device selection. The ControlNet pipeline accepts:
\begin{itemize}
    \item Input sketch preprocessed through binary thresholding and inversion.
    \item Text prompt for generation guidance.
    \item ControlNet conditioning scale (0.7) controlling sketch structure preservation.
    \item Guidance scale (8.0) for prompt adherence.
    \item UniPCMultistepScheduler for efficient 30-step inference.
\end{itemize}
The AI pipeline is loaded in a separate thread to prevent blocking the user interface during initialization, which can take several minutes on first run.

\textbf{Depth Estimation.}
The Depth-Anything-Small model is loaded via Hugging Face Transformers pipeline API. The model processes the generated texture and outputs a depth map, which is resized to match the texture resolution (512x512).

\textbf{Normal Map Computation.}
Normal maps are computed using OpenCV's Sobel operators with kernel size 5:
\begin{equation}
\mathbf{N} = \text{normalize}([-S_x, -S_y, k])
\end{equation}
where $S_x$ and $S_y$ are Sobel gradients, and $k$ is a constant (500.0) representing the base normal component. The result is normalized and encoded to [0, 255] range for texture storage.

\textbf{Roughness Map Computation.}
Roughness maps are derived from albedo textures through grayscale conversion and inversion:
\begin{equation}
R = 255 - \text{normalize}(\text{grayscale}(A))
\end{equation}
where $A$ is the albedo texture. This assumes darker albedo regions correspond to rougher surfaces, providing a physically plausible approximation.

\subsection{System Integration}
The system integrates AI and classical components through a two-mode interface:
\begin{itemize}
    \item \textit{Paint Mode}: 2D sketch creation using PyGame.
    \item \textit{View Mode}: 3D visualization using OpenGL.
\end{itemize}

Mode switching occurs automatically after AI generation completes. The system handles blocking AI operations gracefully through threaded loading and progress callbacks. A project management system saves all generated materials (albedo, depth, normal, roughness) in organized project folders, enabling users to reload previous work. The interface includes template loading, project browsing, and real-time progress feedback during generation.

% ==========================================
% 5. EVALUATION
% ==========================================
\section{Evaluation}

\subsection{Visual Quality Assessment}
We evaluate the visual quality of generated materials through qualitative analysis. Figure~\ref{fig:results} illustrates the generation pipeline results. The system successfully translates a binary sketch into a rich albedo texture using Stable Diffusion, which is then processed into PBR maps to produce a realistic 3D render with depth displacement and lighting.

\begin{figure}[h]
\centering
% 1. Input Sketch
\begin{subfigure}{0.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{figures/sketch_input.png} % Pastikan nama file sesuai
\caption{Input Sketch}
\end{subfigure}
\hfill
% 2. Albedo Map (Hasil Diffusion)
\begin{subfigure}{0.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{figures/albedo.png} % Pastikan nama file sesuai
\caption{Generated Albedo}
\end{subfigure}
\hfill
% 3. Final 3D Render
\begin{subfigure}{0.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{figures/final_render.png} % Ganti dengan nama file screenshot 3D kamu
\caption{Final 3D Render}
\end{subfigure}

\caption{Visual generation process: (a) User input sketch, (b) Albedo texture generated by Stable Diffusion ControlNet, and (c) Final interactive 3D rendering demonstrating displacement and Phong lighting.}
\label{fig:results}
\end{figure}

The Phong lighting model provides realistic illumination, with specular highlights enhanced by roughness map modulation. Displacement mapping creates visible geometric relief based on the depth extracted from the albedo, while normal mapping adds fine-scale surface variation. The combination produces materials that appear three-dimensional and realistic based on the initial 2D input.

\subsection{Performance Metrics}
Performance measurements were conducted on three distinct hardware configurations to evaluate system scalability across server-grade GPUs, consumer silicon, and standard CPUs. The breakdown of generation times is detailed in Table~\ref{tab:performance}.

\begin{table}[h]
\centering
\caption{Generation time for different hardware configurations}
\label{tab:performance}
\footnotesize
\begin{tabularx}{\columnwidth}{lXXXX}
\toprule
\textbf{Hardware} & \textbf{Albedo} & \textbf{Roughness} & \textbf{Depth} & \textbf{Normal} \\
\midrule
NVIDIA RTX 2000 Ada (CUDA) & 5.41s & 0.01s & 0.08s & 0.10s \\
Apple M4 Pro (MPS) & 24.05s & 0.01s & 0.49s & 0.06s \\
Intel i5-11300H (CPU) & 264.00s & 0.03s & 0.08s & 0.15s \\
\bottomrule
\end{tabularx}
\end{table}

The results indicate that the texture synthesis stage (Albedo generation) is the primary computational bottleneck, accounting for over 95\% of the total processing time. The NVIDIA RTX 2000 Ada demonstrates superior performance, completing the entire pipeline in under 6 seconds, which enables a near real-time interactive workflow. The Apple M4 Pro provides a viable experience with $\sim$25-second wait times, while CPU-only execution proves impractical for rapid iteration, requiring over 4 minutes per generation.

Conversely, the auxiliary map generation (Depth, Normal, Roughness) is computationally negligible, taking less than a second even on consumer hardware. Once the assets are generated, the classical OpenGL rendering pipeline maintains a stable 60 FPS on all tested configurations, ensuring smooth visualization and camera interaction regardless of the generation latency.

% ==========================================
% 6. DISCUSSION
% ==========================================
\section{Discussion}

\subsection{Findings}
Our implementation demonstrates that modern AI models can be effectively integrated with classical rendering pipelines to democratize material creation. The combination of ControlNet-based Stable Diffusion for texture generation and learned depth estimation provides a complete PBR material creation workflow from a simple binary sketch.

Key observations based on our evaluation include:
\begin{itemize}
    \item \textbf{Asymmetric Computational Load}: The pipeline workload is heavily skewed. The generative diffusion process (Albedo) consumes over 95\% of the total processing time, whereas the geometry extraction (Depth, Normal, Roughness) is computationally trivial, executing in under 0.5 seconds on modern silicon.
    \item \textbf{Hardware Scalability}: The system exhibits dramatic scalability. On server-grade hardware (RTX 2000 Ada), the system approaches real-time interactivity ($\sim$5s generation), while consumer Apple Silicon (M4 Pro) offers a viable "coffee-break" workflow ($\sim$24s). CPU execution, while functional, remains impractical for iterative design.
    \item \textbf{Visual Fidelity vs. Input}: ControlNet successfully preserves the structural intent of the user's sketch (e.g., brick patterns, tile layouts) while the classical rendering engine ensures that the resulting 3D visualization reacts realistically to lighting changes, verifying the effectiveness of the generated PBR maps.
    \item \textbf{Rendering Stability}: Regardless of the generation time, the visualization module maintains a steady 60 FPS, proving that the decoupling of asset generation (AI) and asset visualization (OpenGL) is critical for user experience.
\end{itemize}

\subsection{Limitations}
Despite the successful integration, several limitations remain:

\textbf{Generation Latency on Consumer Hardware.}
While high-end GPUs offer a seamless experience, the ~24-second wait time on the Apple M4 Pro (a high-end consumer chip) indicates that the system still poses friction for rapid iteration on standard laptops without dedicated CUDA GPUs.

\textbf{Resolution Constraints.}
Textures are currently fixed at 512x512 resolution due to the inherent constraints of the Stable Diffusion v1.5 model used. Upscaling would require additional super-resolution steps, further increasing generation time.

\textbf{Lighting Flexibility.}
The current rendering implementation uses a fixed single-point lighting setup (plus ambient and rim). While the generated maps support full PBR, the visualization tool does not yet allow users to dynamically place lights or change environment maps to test the material under different conditions.

% ==========================================
% 7. FUTURE WORK
% ==========================================
\section{Future Work}

Several directions can guide future improvements to optimize the system for broader adoption:

\textbf{Latency Optimization via Distillation.}
Since Albedo generation is the sole bottleneck, future iterations could employ Latent Consistency Models (LCM) or SDXL Turbo. These models can reduce the inference steps from 30 to roughly 4-8 steps, potentially bringing the generation time on consumer hardware down to sub-5 second levels.

\textbf{Interactive Lighting Controls.}
To better validate the generated roughness and normal maps, the visualization engine should be upgraded to support mouse-driven light positioning or Image-Based Lighting (IBL) with HDRI maps. This would allow users to verify specular responses more intuitively.

\textbf{Texture Tiling and Export.}
Currently, the generated textures are not guaranteed to be perfectly tileable. Integrating a "seamless tiling" post-processing step (via circular convolution in the diffusion process) and adding an OBJ/MTL export feature would allow these materials to be directly used in game engines like Unity or Unreal Engine.

% ==========================================
% 8. CONCLUSION
% ==========================================
\section{Conclusion}

We have presented \textit{Sketch-to-Material Studio}, a system that successfully bridges the gap between AI-driven texture generation and classical 3D rendering. Addressing the limitations of prior generative methods which often lack geometric context, our approach effectively integrates Stable Diffusion ControlNet with learned depth estimation to produce comprehensive PBR material sets. By combining the creative flexibility of diffusion models with the precision of classical OpenGL rasterization, we ensure that generated textures not only preserve the user's artistic intent from the original sketch but are also immediately usable in a realistic lighting environment.

Our evaluation demonstrates that while the texture synthesis phase remains computationally intensive on consumer hardware, the subsequent map extraction and classical rendering pipeline provide a highly efficient and interactive visualization experience. The system effectively automates the technically demanding aspects of material creation—such as depth displacement and normal mapping—allowing users to focus purely on the creative design process. This combination addresses the challenge of rapid material prototyping in 3D graphics workflows, making material creation more accessible to artists and developers.

% ==========================================
% 9. REFERENCES
% ==========================================
\begin{thebibliography}{99}

\bibitem{phong1975illumination}
Bui Tuong Phong.
\newblock Illumination for computer generated pictures.
\newblock \textit{Communications of the ACM}, 18(6):311--317, 1975.

\bibitem{catmull1974subdivision}
Edwin Catmull.
\newblock A subdivision algorithm for computer display of curved surfaces.
\newblock Technical report, University of Utah, 1974.

\bibitem{cook1984shade}
Robert L. Cook.
\newblock Shade trees.
\newblock In \textit{Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques}, SIGGRAPH '84, pages 223--231, 1984.

\bibitem{blinn1978simulation}
James F. Blinn.
\newblock Simulation of wrinkled surfaces.
\newblock In \textit{Proceedings of the 5th Annual Conference on Computer Graphics and Interactive Techniques}, SIGGRAPH '78, pages 286--292, 1978.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \textit{Advances in Neural Information Processing Systems}, volume 27, 2014.

\bibitem{kingma2013auto}
Diederik P. Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \textit{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \textit{Advances in Neural Information Processing Systems}, volume 33, pages 6840--6851, 2020.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{zhang2023adding}
Lvmin Zhang and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.
\newblock \textit{arXiv preprint arXiv:2302.05543}, 2023.

\bibitem{saxena2009make3d}
Ashutosh Saxena, Min Sun, and Andrew Y. Ng.
\newblock Make3d: Learning 3d scene structure from a single still image.
\newblock \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 31(5):824--840, 2009.

\bibitem{eigen2015predicting}
David Eigen, Christian Puhrsch, and Rob Fergus.
\newblock Depth map prediction from a single image using a multi-scale deep network.
\newblock In \textit{Advances in Neural Information Processing Systems}, volume 27, 2015.

\bibitem{godard2017unsupervised}
Clément Godard, Oisin Mac Aodha, and Gabriel J. Brostow.
\newblock Unsupervised monocular depth estimation with left-right consistency.
\newblock In \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 270--279, 2017.

\bibitem{yang2024depth}
Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao.
\newblock Depth anything: Unleashing the power of large-scale unlabeled data.
\newblock \textit{arXiv preprint arXiv:2401.10891}, 2024.

\bibitem{horn1981determining}
Berthold K. P. Horn.
\newblock Determining lightness from an image.
\newblock \textit{Computer Graphics and Image Processing}, 3(4):277--299, 1981.

\bibitem{mildenhall2020nerf}
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.
\newblock NeRF: Representing scenes as neural radiance fields for view synthesis.
\newblock In \textit{European Conference on Computer Vision}, pages 405--421, 2020.

\end{thebibliography}

\end{document}

