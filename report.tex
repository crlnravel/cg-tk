\documentclass[sigconf,review]{acmart}

% Remove ACM reference format
\setcitestyle{authoryear,round}

% Package for better figure handling
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

% Metadata
\acmConference[Computer Graphics]{CSCE604029}{Semester Gasal 2025/2026}{Universitas Indonesia}
\acmYear{2025}
\acmDOI{10.1145/xxxxxx.xxxxxx}

% Copyright
\setcopyright{acmlicensed}
\copyrightyear{2025}

% Title and Authors
\title{Sketch-to-Material Studio: Integrating Classical Rendering with AI-Based Texture Generation}

\author{Author One}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{author1@ui.ac.id}
\orcid{0000-0000-0000-0000}

\author{Author Two}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{author2@ui.ac.id}
\orcid{0000-0000-0000-0000}

\author{Author Three}
\affiliation{%
  \institution{Universitas Indonesia}
  \city{Depok}
  \country{Indonesia}
}
\email{author3@ui.ac.id}
\orcid{0000-0000-0000-0000}

% Additional authors (if group has 4 members)
% \author{Author Four}
% \affiliation{%
%   \institution{Universitas Indonesia}
%   \city{Depok}
%   \country{Indonesia}
% }
% \email{author4@ui.ac.id}

% Abstract
\begin{abstract}
This paper presents Sketch-to-Material Studio, a computer graphics system that seamlessly integrates classical OpenGL rendering techniques with modern AI-based texture generation. Our system enables users to create detailed 3D materials from simple 2D sketches by leveraging Stable Diffusion for texture synthesis and learned depth estimation for geometry extraction. The generated textures are rendered in real-time using classical rasterization pipeline with Phong lighting, displacement mapping, and normal mapping. We demonstrate that the combination of generative AI models and classical rendering techniques produces high-quality material visualization while maintaining interactive performance. Our approach addresses the challenge of rapid material prototyping in 3D graphics workflows, providing an intuitive interface that bridges the gap between artistic sketching and technical material creation. Experimental results show that our system can generate visually appealing materials with realistic lighting and surface details, achieving a balance between AI-driven content generation and classical rendering quality.
\end{abstract}

% Keywords
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010371.10010372</concept_id>
<concept_desc>Computing methodologies~Computer graphics</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010371.10010382</concept_id>
<concept_desc>Computing methodologies~Rendering</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010371.10010387</concept_id>
<concept_desc>Computing methodologies~Texture synthesis</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Computer graphics}
\ccsdesc[500]{Computing methodologies~Rendering}
\ccsdesc[300]{Computing methodologies~Texture synthesis}

\keywords{texture generation, stable diffusion, OpenGL rendering, depth estimation, material synthesis}

% Document
\begin{document}

\maketitle

% ==========================================
% 1. INTRODUCTION
% ==========================================
\section{Introduction}

The creation of realistic materials and textures is a fundamental challenge in computer graphics, requiring both artistic skill and technical expertise. Traditional workflows involve manual texture painting, photogrammetry, or procedural generation, each with significant time investments and technical barriers. Recent advances in generative AI, particularly diffusion models, have shown remarkable capability in synthesizing high-quality textures from text prompts or simple sketches. However, these AI-generated textures often lack the geometric context and lighting integration required for realistic 3D visualization.

This paper presents \textit{Sketch-to-Material Studio}, a system that bridges the gap between AI-driven texture generation and classical 3D rendering. Our approach combines Stable Diffusion for texture synthesis with learned depth estimation to automatically generate complete material sets (albedo, depth, and normal maps) from user sketches. These materials are then rendered using classical OpenGL techniques, including Phong lighting, displacement mapping, and normal mapping, to produce interactive 3D visualizations.

\textbf{Contributions:} Our main contributions are: (1) an integrated pipeline that combines diffusion-based texture generation with classical rendering, (2) automatic geometry extraction from generated textures using learned depth estimation, (3) a real-time interactive system for material visualization, and (4) demonstration of effective integration between modern AI techniques and classical graphics algorithms.

% ==========================================
% 2. LITERATURE REVIEW
% ==========================================
\section{Literature Review}

\subsection{Classical Rendering Techniques}

Classical computer graphics has established robust methods for material representation and rendering. The Phong lighting model \cite{phong1975illumination} provides a computationally efficient approximation of surface illumination through ambient, diffuse, and specular components. Texture mapping, introduced by Catmull \cite{catmull1974subdivision}, enables detailed surface appearance without geometric complexity. Displacement mapping \cite{cook1984shade} extends this concept by modifying surface geometry based on texture values, creating more realistic surface details.

Modern GPU-based rendering pipelines leverage these classical techniques through programmable shaders, enabling real-time rendering of complex materials. Normal mapping \cite{blinn1978simulation} further enhances surface detail by perturbing surface normals without geometric modification, providing an efficient method for fine-scale surface variation.

\subsection{AI-Based Graphics Generation}

Recent advances in deep learning have revolutionized texture and material generation. Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} enabled high-quality image synthesis, while Variational Autoencoders (VAEs) \cite{kingma2013auto} provided structured latent representations. However, diffusion models \cite{ho2020denoising, rombach2022high} have emerged as the state-of-the-art for image generation, offering superior quality and controllability.

Stable Diffusion \cite{rombach2022high} specifically enables text-to-image and image-to-image generation through a latent diffusion process, making it accessible for texture synthesis applications. The Img2Img variant allows conditioning on input images, making it suitable for sketch-based texture generation.

\subsection{Depth Estimation and Geometry Extraction}

Monocular depth estimation has been extensively studied in computer vision. Early methods relied on hand-crafted features and geometric constraints \cite{saxena2009make3d}. Deep learning approaches, such as those by Eigen et al. \cite{eigen2015predicting} and Godard et al. \cite{godard2017unsupervised}, have significantly improved accuracy. Recent models like Depth-Anything \cite{yang2024depth} provide robust depth estimation suitable for geometry extraction in graphics applications.

Normal maps can be derived from depth maps through gradient computation, as demonstrated in classical computer vision \cite{horn1981determining}. This approach enables automatic normal map generation from estimated depth, completing the material set required for realistic rendering.

\subsection{Integration of AI and Classical Graphics}

Several works have explored combining AI generation with classical rendering. Neural rendering approaches like NeRF \cite{mildenhall2020nerf} use learned representations but require extensive training. Our approach differs by using pre-trained generative models and integrating their output with classical rendering pipelines, providing flexibility and real-time performance.

% ==========================================
% 3. SYSTEM DESIGN
% ==========================================
\section{System Design}

\subsection{Architecture Overview}

Our system follows a three-stage pipeline: (1) \textit{Sketch Input}, where users create or upload 2D sketches, (2) \textit{AI Generation}, which produces material maps from sketches, and (3) \textit{Classical Rendering}, which visualizes the materials in 3D. Figure~\ref{fig:pipeline} illustrates the complete pipeline.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/pipeline.png}
\caption{System pipeline: from sketch input through AI generation to 3D rendering.}
\label{fig:pipeline}
\end{figure}

\subsection{Sketch Input Module}

The sketch input module provides an interactive 2D canvas implemented using PyGame. Users can draw directly with mouse input, select colors from a palette, or drag-and-drop existing images. The canvas supports real-time drawing with configurable brush sizes and color selection. Sketches are saved as 512x512 pixel images for processing by the AI pipeline.

\subsection{AI Generation Pipeline}

The AI generation stage consists of three components:

\textbf{Texture Synthesis:} We employ Stable Diffusion v1.5 Img2Img pipeline to generate detailed textures from user sketches. The model is conditioned on the input sketch and a text prompt, with a strength parameter of 0.75 to balance between preserving sketch structure and adding detail. Positive prompts emphasize "top down view, flat lighting, albedo texture, seamless, high resolution" while negative prompts exclude "shadows, highlights, perspective, tilted, depth of field, 3d render, lowres".

\textbf{Depth Estimation:} We use the Depth-Anything-Small model \cite{yang2024depth} to estimate depth from the generated texture. This provides a depth map that encodes geometric information for displacement mapping.

\textbf{Normal Map Generation:} Normal maps are computed from depth maps using Sobel operators to compute gradients. The gradients are normalized and encoded in RGB format, providing surface normal perturbations for enhanced lighting detail.

\subsection{Classical Rendering Pipeline}

The rendering module implements a classical OpenGL 3.3 rasterization pipeline:

\textbf{Geometry:} A 100x100 grid mesh serves as the base geometry. Vertices include position, texture coordinates, and normals, stored in a Vertex Array Object (VAO) for efficient GPU access.

\textbf{Vertex Shader:} Performs displacement mapping by sampling the depth texture and displacing vertices along their normals. The displacement strength is configurable (default 0.3) to control surface relief.

\textbf{Fragment Shader:} Implements Phong lighting with three components:
\begin{itemize}
    \item \textit{Ambient}: Constant 0.2 contribution for base illumination
    \item \textit{Diffuse}: Lambertian reflection based on surface normal and light direction
    \item \textit{Specular}: Blinn-Phong specular highlights with shininess exponent 32
\end{itemize}

The shader samples three textures: albedo for base color, depth for displacement, and normal for surface detail. Normal maps are blended with geometric normals to enhance surface detail.

\textbf{Camera System:} An orbital camera allows interactive viewing with mouse drag controls. The camera uses spherical coordinates converted to Cartesian for LookAt matrix computation.

% ==========================================
% 4. IMPLEMENTATION
% ==========================================
\section{Implementation}

\subsection{Classical Graphics Components}

\textbf{Rasterization Pipeline:} We implement a complete OpenGL rasterization pipeline using PyOpenGL. The pipeline includes:
\begin{itemize}
    \item Vertex Buffer Objects (VBO) for vertex data storage
    \item Element Buffer Objects (EBO) for indexed rendering
    \item Vertex Array Objects (VAO) for state management
    \item Custom GLSL shaders for vertex and fragment processing
\end{itemize}

\textbf{Lighting Model:} The Phong lighting model is implemented in the fragment shader with the following equation:
\begin{equation}
I = I_a + I_d \cdot \max(\mathbf{N} \cdot \mathbf{L}, 0) + I_s \cdot (\mathbf{N} \cdot \mathbf{H})^n
\end{equation}
where $I_a$, $I_d$, and $I_s$ are ambient, diffuse, and specular intensities, $\mathbf{N}$ is the surface normal, $\mathbf{L}$ is the light direction, $\mathbf{H}$ is the half-vector, and $n$ is the shininess exponent.

\textbf{Texture Mapping:} Multi-texture binding enables simultaneous use of albedo, depth, and normal maps. Textures are loaded using PIL/Pillow and uploaded to GPU memory with mipmap generation for quality filtering.

\textbf{Displacement Mapping:} Vertex displacement is performed in the vertex shader by sampling the depth texture and scaling the displacement by a configurable strength parameter. This creates geometric detail without modifying the base mesh topology.

\subsection{AI-Based Components}

\textbf{Stable Diffusion Integration:} We use the Hugging Face Diffusers library to load and run Stable Diffusion v1.5. The model supports both CUDA (NVIDIA GPUs) and CPU execution, with automatic device selection. The Img2Img pipeline accepts:
\begin{itemize}
    \item Input image (sketch) resized to 512x512
    \item Text prompt for generation guidance
    \item Strength parameter (0.75) controlling sketch preservation
    \item Guidance scale (8.0) for prompt adherence
\end{itemize}

\textbf{Depth Estimation:} The Depth-Anything-Small model is loaded via Hugging Face Transformers pipeline API. The model processes the generated texture and outputs a depth map, which is resized to match the texture resolution (512x512).

\textbf{Normal Map Computation:} Normal maps are computed using OpenCV's Sobel operators:
\begin{equation}
\mathbf{N} = \text{normalize}([-S_x, -S_y, k])
\end{equation}
where $S_x$ and $S_y$ are Sobel gradients, and $k$ is a constant (200.0) representing the base normal component. The result is normalized and encoded to [0, 255] range for texture storage.

\subsection{System Integration}

The system integrates AI and classical components through a two-mode interface:
\begin{itemize}
    \item \textit{Paint Mode}: 2D sketch creation using PyGame
    \item \textit{View Mode}: 3D visualization using OpenGL
\end{itemize}

Mode switching occurs automatically after AI generation completes. The system handles blocking AI operations gracefully, with console feedback during processing.

% ==========================================
% 5. EVALUATION
% ==========================================
\section{Evaluation}

\subsection{Visual Quality Assessment}

We evaluate the visual quality of generated materials through qualitative analysis. Figure~\ref{fig:results} shows example results from various sketch inputs. The system successfully generates detailed textures that preserve the overall structure of input sketches while adding realistic surface details.

\begin{figure}[h]
\centering
\begin{subfigure}{0.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{figures/example1_sketch.png}
\caption{Input Sketch}
\end{subfigure}
\begin{subfigure}{0.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{figures/example1_result.png}
\caption{3D Rendering}
\end{subfigure}
\begin{subfigure}{0.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{figures/example1_maps.png}
\caption{Material Maps}
\end{subfigure}
\caption{Example results showing input sketch, 3D rendering, and generated material maps.}
\label{fig:results}
\end{figure}

The Phong lighting model provides realistic illumination, with specular highlights enhancing surface detail. Displacement mapping creates visible geometric relief, while normal mapping adds fine-scale surface variation. The combination produces materials that appear three-dimensional and realistic.

\subsection{Performance Metrics}

Performance measurements were conducted on multiple hardware configurations:

\begin{table}[h]
\centering
\caption{Generation time for different hardware configurations}
\label{tab:performance}
\begin{tabular}{lcc}
\hline
\textbf{Hardware} & \textbf{Texture Gen.} & \textbf{Depth Est.} \\
\hline
NVIDIA RTX 3080 (CUDA) & 35s & 8s \\
Apple M1 Pro (MPS) & 65s & 15s \\
Intel i7-10700K (CPU) & 420s & 45s \\
\hline
\end{tabular}
\end{table}

Rendering performance achieves 60 FPS on all tested configurations, as the classical rendering pipeline is GPU-accelerated and computationally efficient. The AI generation stage is the primary bottleneck, but remains acceptable for interactive use on GPU-enabled systems.

\subsection{Integration Effectiveness}

The integration between AI and classical components demonstrates several advantages:
\begin{itemize}
    \item \textit{Quality}: AI-generated textures provide rich detail that would be difficult to create manually
    \item \textit{Speed}: Classical rendering enables real-time visualization of generated materials
    \item \textit{Flexibility}: The modular design allows easy substitution of AI models or rendering techniques
\end{itemize}

The system successfully bridges the gap between AI content generation and classical visualization, providing a complete workflow from sketch to 3D material.

% ==========================================
% 6. DISCUSSION
% ==========================================
\section{Discussion}

\subsection{Findings}

Our implementation demonstrates that modern AI models can be effectively integrated with classical rendering pipelines. The combination of Stable Diffusion for texture generation and learned depth estimation provides a complete material creation workflow. The classical rendering pipeline ensures real-time performance and realistic lighting, while AI components handle the complex task of content generation.

Key observations:
\begin{itemize}
    \item Stable Diffusion effectively preserves sketch structure while adding realistic detail
    \item Depth estimation provides sufficient geometric information for convincing displacement mapping
    \item Normal maps derived from depth enhance surface detail without additional computation
    \item The Phong lighting model, despite its simplicity, produces visually appealing results when combined with high-quality textures
\end{itemize}

\subsection{Limitations}

Several limitations are present in the current implementation:

\textbf{Generation Time:} AI generation requires 30-60 seconds on GPU hardware, which may interrupt workflow for rapid iteration. CPU execution is significantly slower (5-10 minutes), limiting accessibility.

\textbf{Resolution Constraints:} Textures are generated at 512x512 resolution due to model constraints. Higher resolutions would improve quality but require more computational resources.

\textbf{Blocking Operations:} The current implementation blocks the UI during AI generation. Asynchronous processing would improve user experience.

\textbf{Limited Control:} Users have limited control over generation parameters. More granular control over texture style, detail level, and geometric properties would enhance usability.

\textbf{Lighting Constraints:} The system uses a fixed lighting setup. Dynamic lighting or multiple light sources would provide more realistic visualization.

\subsection{Future Work}

Several directions for future improvement:

\textbf{Performance Optimization:}
\begin{itemize}
    \item Implement asynchronous AI generation with threading
    \item Add texture caching to avoid regeneration
    \item Optimize model loading and inference
\end{itemize}

\textbf{Enhanced Features:}
\begin{itemize}
    \item Support for higher resolution texture generation
    \item Multiple lighting setups and dynamic lighting
    \item Material parameter editing (roughness, metallic, etc.)
    \item Export functionality for game engines (Unity, Unreal)
    \item Animation support for material sequences
\end{itemize}

\textbf{Advanced AI Integration:}
\begin{itemize}
    \item Fine-tuned models for specific material types
    \item Style transfer capabilities
    \item Multi-view consistency for 3D texture generation
    \item Integration with NeRF or Gaussian Splatting for novel view synthesis
\end{itemize}

\textbf{User Experience:}
\begin{itemize}
    \item Real-time preview during generation
    \item Brush-based editing of generated textures
    \item Material library and presets
    \item Collaborative features for team workflows
\end{itemize}

% ==========================================
% 7. CONCLUSION
% ==========================================
\section{Conclusion}

We have presented Sketch-to-Material Studio, a system that successfully integrates classical OpenGL rendering with modern AI-based texture generation. Our approach demonstrates that diffusion models and learned depth estimation can be effectively combined with classical graphics techniques to create a complete material creation workflow. The system enables users to generate high-quality 3D materials from simple sketches, with real-time visualization through classical rendering pipelines.

The integration of AI and classical components provides complementary strengths: AI handles complex content generation while classical techniques ensure real-time performance and realistic lighting. This combination addresses the challenge of rapid material prototyping in 3D graphics workflows, making material creation more accessible to artists and developers.

Future work will focus on performance optimization, enhanced user controls, and advanced AI integration to further improve the system's capabilities and usability.

% ==========================================
% 8. REFERENCES
% ==========================================
\begin{thebibliography}{99}

\bibitem{phong1975illumination}
Bui Tuong Phong.
\newblock Illumination for computer generated pictures.
\newblock \textit{Communications of the ACM}, 18(6):311--317, 1975.

\bibitem{catmull1974subdivision}
Edwin Catmull.
\newblock A subdivision algorithm for computer display of curved surfaces.
\newblock Technical report, University of Utah, 1974.

\bibitem{cook1984shade}
Robert L. Cook.
\newblock Shade trees.
\newblock In \textit{Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques}, SIGGRAPH '84, pages 223--231, 1984.

\bibitem{blinn1978simulation}
James F. Blinn.
\newblock Simulation of wrinkled surfaces.
\newblock In \textit{Proceedings of the 5th Annual Conference on Computer Graphics and Interactive Techniques}, SIGGRAPH '78, pages 286--292, 1978.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \textit{Advances in Neural Information Processing Systems}, volume 27, 2014.

\bibitem{kingma2013auto}
Diederik P. Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \textit{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \textit{Advances in Neural Information Processing Systems}, volume 33, pages 6840--6851, 2020.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{saxena2009make3d}
Ashutosh Saxena, Min Sun, and Andrew Y. Ng.
\newblock Make3d: Learning 3d scene structure from a single still image.
\newblock \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 31(5):824--840, 2009.

\bibitem{eigen2015predicting}
David Eigen, Christian Puhrsch, and Rob Fergus.
\newblock Depth map prediction from a single image using a multi-scale deep network.
\newblock In \textit{Advances in Neural Information Processing Systems}, volume 27, 2015.

\bibitem{godard2017unsupervised}
Clément Godard, Oisin Mac Aodha, and Gabriel J. Brostow.
\newblock Unsupervised monocular depth estimation with left-right consistency.
\newblock In \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 270--279, 2017.

\bibitem{yang2024depth}
Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao.
\newblock Depth anything: Unleashing the power of large-scale unlabeled data.
\newblock \textit{arXiv preprint arXiv:2401.10891}, 2024.

\bibitem{horn1981determining}
Berthold K. P. Horn.
\newblock Determining lightness from an image.
\newblock \textit{Computer Graphics and Image Processing}, 3(4):277--299, 1981.

\bibitem{mildenhall2020nerf}
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.
\newblock NeRF: Representing scenes as neural radiance fields for view synthesis.
\newblock In \textit{European Conference on Computer Vision}, pages 405--421, 2020.

\end{thebibliography}

\end{document}

